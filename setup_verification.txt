1. Output of `java -version`

jjvil@DESKTOP-BKBS4MB:~$ java -version
openjdk version "11.0.30" 2026-01-20
OpenJDK Runtime Environment (build 11.0.30+7-post-Ubuntu-1ubuntu124.04)
OpenJDK 64-Bit Server VM (build 11.0.30+7-post-Ubuntu-1ubuntu124.04, mixed mode, sharing)

2. Output of `spark-shell --version`

jjvil@DESKTOP-BKBS4MB:~$ spark-shell --version
26/02/23 21:04:06 WARN Utils: Your hostname, DESKTOP-BKBS4MB resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
26/02/23 21:04:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.5.8
      /_/

Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.30
Branch HEAD
Compiled by user runner on 2026-01-12T04:16:44Z
Revision 5a48a37b2dbd7b51e3640cd1d947438459556cc6
Url https://github.com/apache/spark
Type --help for more information.

3. Screenshot or output of the PySpark test (sum of 1-5 = 15)

jjvil@DESKTOP-BKBS4MB:~$ pyspark
Python 3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
26/02/23 21:01:39 WARN Utils: Your hostname, DESKTOP-BKBS4MB resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
26/02/23 21:01:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/02/23 21:01:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.8
      /_/

Using Python version 3.12.3 (main, Jan 22 2026 20:57:42)
Spark context Web UI available at http://10.255.255.254:4040
Spark context available as 'sc' (master = local[*], app id = local-1771902102338).
SparkSession available as 'spark'.
>>> sc.parallelize([1,2,3,4,5]).reduce(lambda a, b: a + b)
15
>>>
>>> quit()

4. Any issues you encountered and how you resolved them

Regarding the extraction process of the .tgz file:
The setup guide gives you commands to run, I am on a Windows system, and the following command to extract the tgz file in Windows Powershell failed: Expand-Archive spark-3.5.0-bin-hadoop3.tgz -DestinationPath C:\spark 
Instead I had to use the following command: "tar -xvzf C:\Users\jjvil\Downloads\spark-3.5.8-bin-hadoop3.tgz" -C "C:\spark", which seemed to solve the extraction process in my case. I also had to create the C:\spark directory to extract it into.

Regarding Environment variables:
The version of PySpark I downloaded differs from the one noted in the instructions. The only one available for me to download from the webpage was spark-3.5.8-bin-hadoop3, but instructions had it noted as spark-3.5.0-bin-hadoop3. Therefore, I had to rename the environment variable according to the file name I downloaded. 
I did not had to create a Java system variable as there was already one from my previous installations. The one set as default was jdk-21, which was not a problem for PySpark to run, thus far, so I left it alone. 

Regarding PySpark installation:
The instructions were not clear where this needed to be installed, on Windows directly or in WSL. I opted to install it on Windows via the Powershell terminal through pip install, that seemed to work. 
  
